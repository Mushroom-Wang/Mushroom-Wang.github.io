---
title: About
permalink: /
layout: page
---

# Intro. 

- I am a PhD student of [Computer Science Department](https://www.cs.purdue.edu/), [Purdue University](https://www.purdue.edu/), and advised by [Suresh Jagannathan](https://www.cs.purdue.edu/homes/suresh/).

- I am working on attack / defense / assurance for deep learning, especially reinforcement learning, from the perspective of formal methods.

- I received my B.Eng of Software Engineering from [the University of Electronic Science and Technology of China](https://www.uestc.edu.cn/) in 2018.

- [CV](https://www.cs.purdue.edu/homes/xiong84/res/cv/cv.pdf) updated at 2021-02-27.

# Contact
```sh
echo "emlrYW5neGlvbmdAZ21haWwuY29t" | base64 -d
```  

# Research Projects and Publications

## Verifiable Reinforcement Learning
Deep neural network is generally used as a black-box function without any formal guarantee on its properties. For example, without formal analysis, we cannot know whether a neural-network-controlled drone will collide with ground. This line of work provided verifiable safety guarantee for neural-network-controlled cyber-physical-systems (e.g., robots, UVA) trained with reinforcement learning.

### Related publications:
*Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning*  
**Zikang Xiong**, and Suresh Jagannathan.  
Under Review [pdf](https://www.cs.purdue.edu/homes/xiong84/res/papers/CAV21.pdf)  

*An Inductive Synthesis Framework for Verifiable Reinforcement Learning.*   
He Zhu, **Zikang Xiong**, Stephen Magill and Suresh Jagannathan.    
PLDI 2019 [pdf](https://arxiv.org/pdf/1907.07273.pdf) [tool](https://github.com/caffett/VRL_CodeReview)  
<span style="color:red"> <em>Distinguished Paper</em> </span>


## Adversarial Attack & Defense of Deep Reinforcement Learning
Neural network controllers are not robust to adversarial attacks, which exposes them in great threat from malicious attackers. Our aim is to explore both attack and defense techniques for deep-neural-network controlled systems, thus providing more robust neural network controllers. 

### Related publications:
*Robustness to Adversarial Attacks in Learning-Enabled Controllers*  
**Zikang Xiong**, Joe Eappen, He Zhu and Suresh Jagannathan.
Under Review [pdf](https://www.cs.purdue.edu/homes/xiong84/res/papers/Adversarial20.pdf) [tool](https://hub.docker.com/repository/docker/caffett/neural_shield)    


# Services
[ADHS 2021](https://sites.uclouvain.be/adhs21/) AEC, [CAV 2020](http://i-cav.org/2020/) AEC, [PLDI 2020](https://conf.researchr.org/home/pldi-2020) AEC



# Services
[ADHS 2021](https://sites.uclouvain.be/adhs21/) AEC, [CAV 2020](http://i-cav.org/2020/) AEC, [PLDI 2020](https://conf.researchr.org/home/pldi-2020) AEC

